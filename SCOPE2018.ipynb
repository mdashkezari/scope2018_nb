{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"top\"></a>\n",
    "\n",
    "## Outline:\n",
    "* [Getting Started](#getting-started)\n",
    "* [Data Structure](#data-structure)\n",
    "* [Catalog](#catalog)\n",
    "<br/>\n",
    "* [**Data Visulization**](#data-visualization)\n",
    "    * [Histogram](#histogram)\n",
    "    * [Regional Map](#map)\n",
    "    * [Time Series](#timeseries)\n",
    "    * [Mutual Trends](#mutual)\n",
    "    * [Section Map](#section)\n",
    "    * [Depth Profile](#depth-profile)\n",
    "    * [Cruise Sampling](#cruise)\n",
    "    * [Amplicon 16s](#amplicon)\n",
    "    * [Colocalize Amplicon](#colocalize-amplicon)\n",
    "    \n",
    "* [**Data Retrieval**](#retrieval)\n",
    "    * [Calling Pre-defined Functions](#retrieval) \n",
    "        * [Space-Time Subset](#space-time)\n",
    "        * [Time Series Subset](#time-series-subset)\n",
    "        * [Depth Profile Subset](#depth-profile-subset)\n",
    "        * [Section Subset](#section-subset)\n",
    "    * [Direct SQL Query](#sql)\n",
    "        * [SQL: Regional Map](#sql-regional)\n",
    "        * [SQL: Time Series](#sql-time-series)\n",
    "        \n",
    "* [**Synthesis Analysis**](#synthesis)\n",
    "    * [Colocalize Custom External Dataset](#external)\n",
    "\n",
    "* [**Use Case**](#use-case)\n",
    "    * [CP-Lyase (Oscar Sosa)](#cplyase)\n",
    "\n",
    "* [**CMAP Online Documentation**](#doc)\n",
    "* [**Open Discussion**](#discussion)\n",
    "* [**Contact**](#contact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"getting-started\"></a>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "\n",
    "<center>\n",
    "<h1> Getting Started </h1>\n",
    "</center>\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "IPython.display.IFrame('https://simons-ocean-atlas-documentation.readthedocs.io/en/latest/getting_started/installation.html', width=1200, height=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"data-structure\"></a>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "\n",
    "<center>\n",
    "<h1> Dataset Strucure</h1>\n",
    "</center>\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "\n",
    "\n",
    "| time        | lat           | lon  | depth | [var1] | [...] | [varn] |\n",
    "| -----------   | -----------   | ----- | ----- | ----- | ----- | ----- |\n",
    "| <%Y-%m-%dT%H:%M:%S>  | [-90, 90] | [-180, 180] | positive number | number | number | number |\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "<center>\n",
    "<h3>    \n",
    "see <a href=https://github.com/mdashkezari/opedia/tree/master/template> here</a> for more details\n",
    "</h3>    \n",
    "</center>\n",
    "\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "<center> <h3> A sample dataset </h3> </center>\n",
    "<center> provided by Katherine Heal <i>et al.</i> (Inglass Lab, UW) </center>\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.read_excel('./data/KM1314_ParticulateCobalamins_2018_06_12_vPublished.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"catalog\"></a>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "\n",
    "<center>\n",
    "<h1> Catalog </h1>\n",
    "</center>\n",
    "\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opedia import getCatalog\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv('./data/catalog.csv')\n",
    "print(df[['Variable', 'Table_Name']].to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"data-visualization\"></a>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "\n",
    "<center>\n",
    "<h1> Data Visualization </h1>\n",
    "</center>\n",
    "\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"histogram\"></a>\n",
    "\n",
    "# Plot Distribution (Satellite, Core Argo Floats)\n",
    "\n",
    "Create histograms of sea surface temperature (satellite), and temperature / salinity measurements by Argo floats.\n",
    "<br/> <br/>\n",
    "**Note:**<br/> \n",
    "* Satellite SST data set is a daily-global product with spatial resolution $\\frac{1}{4}^\\circ \\times \\frac{1}{4}^\\circ$.<br/>\n",
    "\n",
    "* Argo float data set has irregular temporal and spatial resolution. <br/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opedia import plotDist as DIS\n",
    "\n",
    "tables = ['tblSST_AVHRR_OI_NRT', 'tblArgoMerge_REP', 'tblArgoMerge_REP']           # see catalog.csv  for the complete list of tables and variable names    \n",
    "variables = ['sst', 'argo_merge_temperature_adj', 'argo_merge_salinity_adj']       # see catalog.csv  for the complete list of tables and variable names\n",
    "startDate = '2016-04-30'   \n",
    "endDate = '2016-04-30'\n",
    "lat1, lat2 = 20, 24\n",
    "lon1, lon2 = -170, 150\n",
    "depth1, depth2 = 0, 1500\n",
    "fname = 'Dist'\n",
    "exportDataFlag = False      # True if you you want to download data\n",
    "\n",
    "DIS.plotDist(tables, variables, startDate, endDate, lat1, lat2, lon1, lon2, depth1, depth2, fname, exportDataFlag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"map\"></a>\n",
    "\n",
    "# Plot Regional Maps (Satellite, Model)\n",
    "\n",
    "Create a regional map using satellite and model data.\n",
    "<br/> <br/>\n",
    "**Notes:**<br/> \n",
    "* Pisces model is a weekly-averaged global model with spatial resolution $\\frac{1}{2}^\\circ \\times \\frac{1}{2}^\\circ$ (data is available only at one-week intervals).<br/>\n",
    "\n",
    "* Satellite SST data set is a daily-global product with spatial resolution $\\frac{1}{4}^\\circ \\times \\frac{1}{4}^\\circ$.<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opedia import plotRegional as REG\n",
    "\n",
    "\n",
    "tables = ['tblsst_AVHRR_OI_NRT', 'tblPisces_NRT']    # see catalog.csv  for the complete list of tables and variable names\n",
    "variables = ['sst', 'Fe']                            # see catalog.csv  for the complete list of tables and variable names   \n",
    "startDate = '2016-04-30'\n",
    "endDate = '2016-04-30'\n",
    "lat1, lat2 = 10, 70\n",
    "lon1, lon2 = -180, -80\n",
    "depth1, depth2 = 0, 0.5\n",
    "fname = 'regional'\n",
    "exportDataFlag = False       # True if you you want to download data\n",
    "\n",
    "REG.regionalMap(tables, variables, startDate, endDate, lat1, lat2, lon1, lon2, depth1, depth2, fname, exportDataFlag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"timeseries\"></a>\n",
    "\n",
    "# Plot Time Seriese (Model, Satellite)\n",
    "\n",
    "Create time series plots using sattelite and model data.\n",
    "<br/> <br/>\n",
    "**Note:**<br/> \n",
    "* Pisces model is a weekly-averaged global model with spatial resolution $\\frac{1}{2}^\\circ \\times \\frac{1}{2}^\\circ$ (data is available only at one-week intervals).<br/>\n",
    "\n",
    "* Satellite wind data set is a 6-hourly global product with spatial resolution $\\frac{1}{4}^\\circ \\times \\frac{1}{4}^\\circ$.<br/>\n",
    "\n",
    "* Satellite Altimetry data set is a daily-global product with spatial resolution $\\frac{1}{4}^\\circ \\times \\frac{1}{4}^\\circ$.<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opedia import plotTS as TS\n",
    "\n",
    "tables = ['tblSST_AVHRR_OI_NRT', 'tblAltimetry_REP', 'tblPisces_NRT']    # see catalog.csv  for the complete list of tables and variable names\n",
    "variables = ['sst', 'sla', 'NO3']                                        # see catalog.csv  for the complete list of tables and variable names\n",
    "startDate = '2016-03-29'\n",
    "endDate = '2016-05-29'\n",
    "lat1, lat2 = 25, 30\n",
    "lon1, lon2 = -160, -155\n",
    "depth1, depth2 = 0, 5\n",
    "fname = 'TS'\n",
    "exportDataFlag = False                                                   # True if you you want to download data\n",
    "\n",
    "TS.plotTS(tables, variables, startDate, endDate, lat1, lat2, lon1, lon2, depth1, depth2, fname, exportDataFlag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"mutual\"></a>\n",
    "\n",
    "# Plot one dataset against another (Model, Satellite)\n",
    "\n",
    "Create plotXY using sattelite and model data.\n",
    "<br/> <br/>\n",
    "**Note:**<br/> \n",
    "* Pisces model is a weekly-averaged global model with spatial resolution $\\frac{1}{2}^\\circ \\times \\frac{1}{2}^\\circ$ (data is available only at one-week intervals).<br/>\n",
    "\n",
    "* Satellite wind data set is a 6-hourly global product with spatial resolution $\\frac{1}{4}^\\circ \\times \\frac{1}{4}^\\circ$.<br/>\n",
    "\n",
    "* Satellite Altimetry data set is a daily-global product with spatial resolution $\\frac{1}{4}^\\circ \\times \\frac{1}{4}^\\circ$.<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opedia import plotXY as XY\n",
    "\n",
    "tables = ['tblSST_AVHRR_OI_NRT', 'tblAltimetry_REP', 'tblPisces_NRT']    # see catalog.csv  for the complete list of tables and variable names\n",
    "variables = ['sst', 'sla', 'NO3']                                        # see catalog.csv  for the complete list of tables and variable names\n",
    "startDate = '2015-03-29'\n",
    "endDate = '2016-03-29'\n",
    "lat1, lat2 = 35, 40\n",
    "lon1, lon2 = -160, -155\n",
    "depth1, depth2 = 0, 5\n",
    "fname = 'XY'\n",
    "exportDataFlag = False                                                   # True if you you want to download data\n",
    "\n",
    "XY.plotXY(tables, variables, startDate, endDate, lat1, lat2, lon1, lon2, depth1, depth2, fname, exportDataFlag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"section\"></a>\n",
    "\n",
    "# Plot Section Map (Model outputs)\n",
    "\n",
    "Create section maps using Darwin and PISCES model outputs.\n",
    "<br/> <br/>\n",
    "**Notes:**\n",
    "* Darwin_Climatology is a monthly climatology version of the Darwin model with spatial resolution $\\frac{1}{2}^\\circ \\times \\frac{1}{2}^\\circ$.<br/>\n",
    "\n",
    "* Pisces model is a weekly-averaged global model with spatial resolution $\\frac{1}{2}^\\circ \\times \\frac{1}{2}^\\circ$ (data is available only at one-week intervals).<br/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opedia import plotSection as SEC\n",
    "\n",
    "tables = ['tblDarwin_Nutrient_Climatology', 'tblPisces_NRT']     # see catalog.csv  for the complete list of tables and variable names      \n",
    "variables = ['CDOM_darwin_clim', 'Fe']                           # see catalog.csv  for the complete list of tables and variable names\n",
    "startDate = '2016-04-30'                                         # PISCES is a weekly model, and here we are using monthly climatology of Darwin model\n",
    "endDate = '2016-04-30'\n",
    "lat1, lat2 = 20, 55\n",
    "lon1, lon2 = -159, -157\n",
    "depth1, depth2 = 0, 6000\n",
    "fname = 'SEC'\n",
    "exportDataFlag = False                                           # True if you you want to download data\n",
    "\n",
    "SEC.sectionMap(tables, variables, startDate, endDate, lat1, lat2, lon1, lon2, depth1, depth2, fname, exportDataFlag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"depth-profile\"></a>\n",
    "\n",
    "# Plot Depth Profile (BGC-Argo Floats, Model outputs)\n",
    "\n",
    "Create depth profile plots using model and BGC-Argo float profiles.\n",
    "<br/> <br/>\n",
    "**Notes:**\n",
    "* Darwin_Climatology is a monthly climatology version of the Darwin model with spatial resolution $\\frac{1}{2}^\\circ \\times \\frac{1}{2}^\\circ$.<br/>\n",
    "\n",
    "* Argo float data set has irregular temporal and spatial resolution. <br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opedia import plotDepthProfile as DEP\n",
    "\n",
    "tables = ['tblArgoMerge_REP', 'tblDarwin_Chl_Climatology']     # see catalog.csv  for the complete list of tables and variable names      \n",
    "variables = ['argo_merge_chl_adj', 'chl01_darwin_clim']        # see catalog.csv  for the complete list of tables and variable names\n",
    "startDate = '2016-04-30'   \n",
    "endDate = '2016-04-30'\n",
    "lat1, lat2 = 20, 24\n",
    "lon1, lon2 = -170, -160\n",
    "depth1, depth2 = 0, 1500\n",
    "fname = 'DEP'\n",
    "exportDataFlag = False                                         # True if you you want to download data\n",
    "\n",
    "DEP.plotDepthProfile(tables, variables, startDate, endDate, lat1, lat2, lon1, lon2, depth1, depth2, fname, exportDataFlag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/> <br/>\n",
    "<a class=\"anchor\" id=\"cruise\"></a>\n",
    "\n",
    "# Colocalize Darwin model and satellite data with cruise\n",
    "\n",
    "Compare the underway (in-situ) picoeukaryote abundance measurements performed during the \"Gradient1.0\" cruise with satellite chlorophyll data and picoeukaryote climatological estimates provided by Darwin model.\n",
    "\n",
    "<br/> \n",
    "**Notes:**<br/> \n",
    "\n",
    "* In-Situ picoeukaryote abundance measurements are results of the SeaFlow data set with 3-minute temporal resultion and irregular spatial resultion.\n",
    "\n",
    "* Satellite Chlorophyll data used in this example is a daily-global reprocessed and optimally interpolated data set with $4~{\\rm km}\\times4~{\\rm km}$ spatial resolution. \n",
    "\n",
    "* Darwin_Climatology is a monthly climatology version of the Darwin model with spatial resolution $\\frac{1}{2}^\\circ \\times \\frac{1}{2}^\\circ$.<br/>\n",
    "\n",
    "<br/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opedia import plotCruise as CRS\n",
    "\n",
    "DB_Cruise = True                 # < True > if cruise trajectory already exists in DB. < False > if arbiturary cruise file (e.g. virtual) \n",
    "source = 'tblSeaFlow'            # cruise table name or path to csv trajectory file    \n",
    "cruise = 'Gradients1.0'              # cruise name, or file name of the csv trajectory file     \n",
    "resampTau = '6H'                 # resample the cruise trajectory making trajectory time-space resolution coarser: e.g. '6H' (6 hourly), '3T' (3 minutes), ... '0' (ignore)  \n",
    "fname = 'alongTrack'             # figure filename\n",
    "tables = ['tblSeaFlow', 'tblDarwin_Plankton_Climatology', 'tblCHL_OI_REP']    # list of varaible table names               \n",
    "variables = ['picoeuk', 'picoeukaryote_c03_darwin_clim', 'chl']               # list of variable names           \n",
    "spatialTolerance = 0.3           # colocalizer spatial tolerance (+/- degrees) \n",
    "exportDataFlag = False           # export the cruise trajectory and colocalized data on disk\n",
    "depth1 = 0                      # depth range start (m) \n",
    "depth2 = 5                       # depth range end (m)  \n",
    "\n",
    "\n",
    "df = CRS.getCruiseTrack(DB_Cruise, source, cruise)\n",
    "df = CRS.resample(df, resampTau) \n",
    "loadedTrack = CRS.plotAlongTrack(tables, variables, cruise, resampTau, df, spatialTolerance, depth1, depth2, fname, exportDataFlag, marker='-', msize=30, clr='darkturquoise')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/> <br/> \n",
    "<a class=\"anchor\" id=\"amplicon\"></a>\n",
    "\n",
    "# Exact Amplicon Sequence Variants (16S) Along Cruise Track\n",
    "### Query by taxonomy level, clustering thereshold, and size fraction\n",
    "\n",
    "The example below retrieves the \"topN\" number of most abundant sequenced organisms along track of the cruise. One can aggregate and visualize the relative abundance of the organisms according to their taxonomy level, clustering levels, and size fractions. The cruise, 'ANT28-5', is an Atlantic latitudinal transect. <br/> <br/>\n",
    "\n",
    "**Thanks to Jed Fuhrman and Jesse McNichol (USC) for the beautiful dataset!**  <br/> <br/> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opedia import esv\n",
    "\n",
    "############## set parameters ################\n",
    "# only plot the top_N number of most abundant organisms\n",
    "topN = 5           \n",
    "# aggregate organisims by their taxa level\n",
    "tax = ['domain', 'kingdom', 'phylum', 'class', 'order', 'genus', 'species'][5]\n",
    "depth1 = 20\n",
    "depth2 = depth1\n",
    "cruise_name = 'ANT28-5'\n",
    "cluster_level = [89, 92, 96, 97, 98, 99, 100][0]        # minimum similarity precentage to be clustred\n",
    "size_frac_lower = [0.2, 3, 8][0]                        # size in micro-meter\n",
    "size_frac_upper = [None, 3, 8][1]                       # size in micro-meter\n",
    "##############################################\n",
    "\n",
    "esv.plotESVs(topN, tax, depth1, depth2, cruise_name, cluster_level, size_frac_lower, size_frac_upper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "<a class=\"anchor\" id=\"colocalize-amplicon\"></a>\n",
    "\n",
    "# Colocalize 16S dataset (or any other) with Model and Satellite\n",
    "\n",
    "Here, the retrieved trends of relative abundances are colocalized with other datasets, in this case with Darwin model. The results are stored in a .csv file in the ./data directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opedia import colocalize as COL\n",
    "\n",
    "DB = False                           # < True > if source data exists in the database. < 0 > if the source data set is a spreadsheet file on disk. \n",
    "source = './data/esv.csv'            # the source table name (or full filename)    \n",
    "temporalTolerance = 3                # colocalizer temporal tolerance (+/- degrees)\n",
    "latTolerance = 0.3                   # colocalizer meridional tolerance (+/- degrees)\n",
    "lonTolerance = 0.3                   # colocalizer zonal tolerance (+/- degrees) \n",
    "depthTolerance = 5                   # colocalizer depth tolerance (+/- meters)\n",
    "tables = ['tblDarwin_Plankton_Climatology', 'tblDarwin_Plankton_Climatology', 'tblDarwin_Plankton_Climatology']    # list of varaible table names               \n",
    "variables = ['prokaryote_c01_darwin_clim', 'prokaryote_c02_darwin_clim', 'cocco_c05_darwin_clim']                  # list of variable names           \n",
    "exportPath = './data/loaded.csv'     # path to save the colocalized data set \n",
    "    \n",
    "COL.matchSource(DB, source, temporalTolerance, latTolerance, lonTolerance, depthTolerance, tables, variables, exportPath)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"retrieval\"></a>\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "<center>\n",
    "<h1> Data Retrieval </h1>\n",
    "<h3> Extract customized subsets of data:  calling pre-defined functions</h3> \n",
    "</center>\n",
    "\n",
    "\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"space-time\"></a>\n",
    "\n",
    "# Space-Time subset\n",
    "This tutorial shows how to retrieve a generic distribution of a variable within a predefined space-time domain. You need to know the variable and table names, both of which can be found in the catalog. Data is retrieved in form of a dataframe with time, space, and variable columns. <br/> <br/> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opedia import subset\n",
    "\n",
    "############## set parameters ################\n",
    "table = 'tblsst_AVHRR_OI_NRT'\n",
    "variable = 'sst'       \n",
    "dt1 = '2016-06-01'\n",
    "dt2 = '2016-06-05'\n",
    "lat1, lat2, lon1, lon2 = 23, 24, -160, -158  \n",
    "depth1, depth2 = 0, 0\n",
    "##############################################\n",
    "\n",
    "subset.spaceTime(table, variable, dt1, dt2, lat1, lat2, lon1, lon2, depth1, depth2)    # retrieves a DataFrame\n",
    "#df.to_csv('data.csv', index=False)      # save the retrieved data into a csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class='anchor' id='time-series-subset'> </a>\n",
    "\n",
    "# Time series subset\n",
    "This tutorial shows how to retrieve time series of a variable within a predefined space-time domain. You need to know the variable and table names, both which can be found in the catalog. The *timeSeries* function computes the mean and standard deviation of the variable per time period. Data is retrieved in form of a dataframe with time, space, and variable columns. <br/> <br/> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opedia import subset\n",
    "\n",
    "############## set parameters ################\n",
    "table = 'tblsst_AVHRR_OI_NRT'\n",
    "variable = 'sst'       \n",
    "dt1 = '2016-06-01'\n",
    "dt2 = '2016-07-01'\n",
    "lat1, lat2, lon1, lon2 = 23, 24, -160, -158  \n",
    "depth1, depth2 = 0, 0\n",
    "##############################################\n",
    "\n",
    "subset.timeSeries(table, variable, dt1, dt2, lat1, lat2, lon1, lon2, depth1, depth2)    # retrieves a DataFrame\n",
    "#df.to_csv('data.csv', index=False)      # save the retrieved data into a csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class='anchor' id='depth-profile-subset'></a>\n",
    "\n",
    "# Depth profile subset\n",
    "This tutorial shows how to retrieve depth profile of a variable within a predefined space-time domain. You need to know the variable and table names, both of which can be found in the catalog. The *depthProfile* function computes the mean and standard deviation of the variable per depth period. Data is retrieved in form of a dataframe. <br/> <br/> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opedia import subset\n",
    "\n",
    "############## set parameters ################\n",
    "table = 'tblPisces_NRT'\n",
    "variable = 'CHL'       \n",
    "dt1 = '2016-04-30'\n",
    "dt2 = '2016-04-30'\n",
    "lat1, lat2, lon1, lon2 = 23, 24, -160, -158  \n",
    "depth1, depth2 = 0, 6000\n",
    "##############################################\n",
    "\n",
    "subset.depthProfile(table, variable, dt1, dt2, lat1, lat2, lon1, lon2, depth1, depth2)    # retrieves a DataFrame\n",
    "#df.to_csv('data.csv', index=False)      # save the retrieved data into a csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class='anchor' id='section-subset'></a>\n",
    "\n",
    "# Section subset\n",
    "This tutorial shows how to retrieve section profile of a variable within a predefined space-time domain. You need to know the variable and table names, both of which can be found in the catalog. Data is retrieved in form of a dataframe with time, space, and variable columns. <br/> <br/> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opedia import subset\n",
    "\n",
    "############## set parameters ################\n",
    "table = 'tblPisces_NRT'\n",
    "variable = 'Fe'       \n",
    "dt1 = '2016-04-30'\n",
    "dt2 = '2016-04-30'\n",
    "lat1, lat2, lon1, lon2 = 22, 50, -160, -158  \n",
    "depth1, depth2 = 0, 6000\n",
    "##############################################\n",
    "\n",
    "subset.section(table, variable, dt1, dt2, lat1, lat2, lon1, lon2, depth1, depth2)    # retrieves a DataFrame\n",
    "#df.to_csv('data.csv', index=False)      # save the retrieved data into a csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"sql\"></a>\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "<center>\n",
    "<h1> Data Retrieval </h1>\n",
    "<h3> Extract customized subsets of data: <u>direct SQL query</u> <h3>      \n",
    "</center>\n",
    "\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class='anchor' id='sql-regional'></a>\n",
    "\n",
    "# SQL: Regional Map\n",
    "If you are familiar with SQL or T-SQL language, you can use \"dbfFetch()\" function to execute any generic query and retrieve data. Below is a simple example showing how to retrieve a snapshot and plot a basic map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opedia import db\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "def plot(dt, lat, lon, data):\n",
    "    plt.imshow(data, extent=[np.min(lon), np.max(lon), np.min(lat), np.max(lat)], origin='bottom', vmin=0, vmax=1e-4)\n",
    "    plt.title(field + '\\n ' + dt1)\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "def prepareQuery(args):\n",
    "    query = \"SELECT [time], lat, lon, depth, %s FROM %s WHERE \"\n",
    "    query += \"[time] BETWEEN'%s' AND '%s' AND \"\n",
    "    query += \"lat BETWEEN %f AND %f AND \"\n",
    "    query += \"lon BETWEEN %f AND %f AND \"\n",
    "    query += \"depth BETWEEN %f AND %f \"\n",
    "    query += \"ORDER BY [time], lat, lon, depth \"\n",
    "    query = query % args\n",
    "    return query \n",
    "\n",
    "\n",
    "\n",
    "############## set parameters ################\n",
    "table = 'tblPisces_NRT'\n",
    "field = 'Fe'        # Mole concentration of dissolved Iron \n",
    "dt1 = '2017-06-03'\n",
    "dt2 = '2017-06-03'\n",
    "lat1, lat2, lon1, lon2 = 10, 55, -180, -100  \n",
    "depth1 = 0\n",
    "depth2 = 1\n",
    "##############################################\n",
    "\n",
    "\n",
    "args = (field, table, dt1, dt2, lat1, lat2, lon1, lon2, depth1, depth2)\n",
    "query = prepareQuery(args)\n",
    "df = db.dbFetch(query)        \n",
    "lat = df.lat.unique()\n",
    "lon = df.lon.unique()\n",
    "shape = (len(lat), len(lon))\n",
    "data = df[field].values.reshape(shape)\n",
    "#df.to_csv(field+'.csv', index=False)    # export data\n",
    "plot(dt1, lat, lon, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class='anchor' id='sql-time-series'></a>\n",
    "\n",
    "# SQL: Time Series\n",
    "If you are familiar with SQL or T-SQL language, you and use \"dbfFetch()\" function to execute any generic query and retrieve data. Below is a simple example showing how to retrieve time series and plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opedia import db\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "def plot(t, y):\n",
    "    plt.plot(t, y, 'o')\n",
    "    plt.xlabel('time')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "def prepareQuery(args):\n",
    "    query = \"SELECT [time], AVG(lat) AS lat, AVG(lon) AS lon, AVG(%s) AS %s FROM %s WHERE \"\n",
    "    query += \"[time] BETWEEN'%s' AND '%s' AND \"\n",
    "    query += \"lat BETWEEN %f AND %f AND \"\n",
    "    query += \"lon BETWEEN %f AND %f \"   \n",
    "    query += \"GROUP BY [time] \"\n",
    "    query += \"ORDER BY [time] \"\n",
    "    query = query % args\n",
    "    return query \n",
    "\n",
    "\n",
    "############## set parameters ################\n",
    "table = 'tblsst_AVHRR_OI_NRT'\n",
    "variable = 'sst'       \n",
    "dt1 = '2016-06-01'\n",
    "dt2 = '2016-10-01'\n",
    "lat1, lat2, lon1, lon2 = 23, 24, -160, -158  \n",
    "##############################################\n",
    "args = (variable, variable, table, dt1, dt2, lat1, lat2, lon1, lon2)\n",
    "query = prepareQuery(args)\n",
    "df = db.dbFetch(query)        \n",
    "#df.to_csv(variable+'.csv', index=False)    # export data\n",
    "plot(df['time'], df[variable])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"synthesis\"></a>\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "<center>\n",
    "<h1> Synthesis Analysis </h1>\n",
    "</center>\n",
    "\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class='anchor' id='external'></a>\n",
    "\n",
    "\n",
    "# Colocalize a custom dataset with Darwin model, and satellite data\n",
    "\n",
    "Colocalize a a custom dataset (Particulate Cobalamins observed on KM1314 cruise) with climatological POC, prokaryote estimates provided by Darwin model, and dissolved iron concentration. The dataset should be in either '.xlsx' or '.csv' format with 'time', 'lat', 'lon', and 'depth' columns. \n",
    "\n",
    "\n",
    "| time        | lat           | lon  | depth | [var1] | [...] | [varn] |\n",
    "| -----------   | -----------   | ----- | ----- | ----- | ----- | ----- |\n",
    "| <%Y-%m-%dT%H:%M:%S>  | [-90, 90] | [-180, 180] | positive number | number | number | number |\n",
    "\n",
    "\n",
    "<br/> \n",
    "**Notes:**<br/> \n",
    "\n",
    "* Darwin_Climatology is a monthly climatology version of the Darwin model with spatial resolution $\\frac{1}{2}^\\circ \\times \\frac{1}{2}^\\circ$.<br/>\n",
    "\n",
    "* Pisces model is a weekly-averaged global model with spatial resolution $\\frac{1}{2}^\\circ \\times \\frac{1}{2}^\\circ$ (data is available only at one-week intervals).<br/>\n",
    "\n",
    "<br/>\n",
    "\n",
    "\n",
    "**Thanks to Anitra Ingalls, Katherine Heal *et al.* (Inglass Lab, UW) for the beautiful dataset!**  <br/> <br/> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from opedia import colocalize as COL\n",
    "\n",
    "DB = False                            # < True > if source data exists in the database. < 0 > if the source data set is a spreadsheet file on disk. \n",
    "source = './data/KM1314_ParticulateCobalamins_2018_06_12_vPublished.xlsx'            # the source table name (or full filename)    \n",
    "temporalTolerance = 1                # colocalizer temporal tolerance (+/- degrees)\n",
    "latTolerance = 0.3                   # colocalizer meridional tolerance (+/- degrees)\n",
    "lonTolerance = 0.3                   # colocalizer zonal tolerance (+/- degrees) \n",
    "depthTolerance = 5                   # colocalizer depth tolerance (+/- meters)\n",
    "tables = ['tblDarwin_Nutrient_Climatology', 'tblPisces_NRT', 'tblDarwin_Plankton_Climatology']    # list of varaible table names               \n",
    "variables = ['poc_darwin_clim', 'Fe', 'prokaryote_c01_darwin_clim']                            # list of variable names           \n",
    "exportPath = './data/loaded.csv'         # path to save the colocalized data set \n",
    "    \n",
    "COL.matchSource(DB, source, temporalTolerance, latTolerance, lonTolerance, depthTolerance, tables, variables, exportPath)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"use-case\"></a>\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "<center>\n",
    "<h1> Use Case </h1>\n",
    "</center>\n",
    "\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class='anchor' id='cplyase'></a>\n",
    "\n",
    "\n",
    "# Colocalize TARA-CPLyase with model nutrient/chemical fields\n",
    "\n",
    "Oscar Sosa *et al.* studied the TARA dataset to investigate the oceanic distribution and abundance of the bacterial carbon-phosphorus (C-P) lyase pathway, a specialized enzyme complex that oxidizes phosphonates to acquire phosphate. A number of environmental parameters from Darwin and PICES models synthesized with this study where dissolved iron demonstrated a strong positive correlation to the CP lyase gene relative abundance, particularly to the metagenomic samples from surface waters.\n",
    "\n",
    "The code below shows the process of colocalizing Oscar's dataset with PISCES model. Since most of the TARA observations carried out before the initiation of PISCES public dataset (year 2012), we had to compute and colocalize the monthly climatology of the model outputs. \n",
    "\n",
    "\n",
    "<br/> \n",
    "**Notes:**<br/> \n",
    "\n",
    "* Pisces model is a weekly-averaged global model with spatial resolution $\\frac{1}{2}^\\circ \\times \\frac{1}{2}^\\circ$ (data is available only at one-week intervals).<br/>\n",
    "\n",
    "<br/>\n",
    "\n",
    "\n",
    "**Thanks to Oscar Sosa *et al.* (Karl Lab, UH)**  <br/> <br/> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from opedia import db\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def appendColumn(df, cols, std):\n",
    "    for col in cols:\n",
    "        df[col] = ''\n",
    "        if std:\n",
    "            df[col+'_std'] = ''\n",
    "    return df\n",
    "\n",
    "def prepareQuery(month, lat1, lat2, lon1, lon2, depth1, depth2):\n",
    "    args = (month, lat1, lat2, lon1, lon2, depth1, depth2)\n",
    "    query = \"SELECT [month], lat, lon, Fe, NO3, O2, PO4, Si, PP, PHYC, CHL FROM tblPisces_NRT WHERE \"\n",
    "    query = query + \"[month]=%s AND \"\n",
    "    query = query + \"lat>=%f AND lat<=%f AND \"\n",
    "    query = query + \"lon>=%f AND lon<=%f AND \"\n",
    "    query = query + \"depth>=%f AND depth<=%f \"\n",
    "    query = query % args\n",
    "    return query\n",
    "\n",
    "\n",
    "def localize(month, lat1, lat2, lon1, lon2, depth1, depth2):\n",
    "    query = prepareQuery(month, lat1, lat2, lon1, lon2, depth1, depth2)\n",
    "    df = db.dbFetch(query)        \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "margin = 0.3                                # lat/lon tolerance (+/- 0.3 deg)\n",
    "depth_margin = 5                            # depth tolerance (+/- 5 m)\n",
    "df = pd.read_csv('./Tara_CPlyase.csv')      # load originaldata set\n",
    "\n",
    "cols = ['month', 'lat1', 'lat2', 'lon1', 'lon2', 'depth1', 'depth2']\n",
    "std = False\n",
    "df = appendColumn(df, cols, std)\n",
    "\n",
    "cols = ['Fe', 'NO3', 'Model_O2', 'PO4', 'Si', 'PP', 'PHYC', 'CHL']\n",
    "std = True\n",
    "df = appendColumn(df, cols, std)\n",
    "\n",
    "for i in ange(len(df)):\n",
    "    month = str(df.time[i]).split(\"/\")[0]\n",
    "    lat = df.lat[i]\n",
    "    lon = df.lon[i]\n",
    "    depth = df.depth[i]\n",
    "    lat1, lat2 = lat - margin, lat + margin\n",
    "    lon1, lon2 = lon - margin, lon + margin\n",
    "    depth1, depth2 = depth - depth_margin, depth + depth_margin\n",
    "    data = localize(month, lat1, lat2, lon1, lon2, depth1, depth2)\n",
    "    df['month'][i], df['lat1'][i], df['lat2'][i], df['lon1'][i], df['lon2'][i], df['depth1'][i], df['depth2'][i] = month, lat1, lat2, lon1, lon2, depth1, depth2\n",
    "    df['Fe'][i], df['Fe_std'][i] = np.mean(data.Fe), np.std(data.Fe)\n",
    "    df['NO3'][i], df['NO3_std'][i] = np.mean(data.NO3), np.std(data.NO3)\n",
    "    df['Model_O2'][i], df['Model_O2_std'][i] = np.mean(data.O2), np.std(data.O2)\n",
    "    df['PO4'][i], df['PO4_std'][i] = np.mean(data.PO4), np.std(data.PO4)\n",
    "    df['Si'][i], df['Si_std'][i] = np.mean(data.Si), np.std(data.Si)\n",
    "    df['PP'][i], df['PP_std'][i] = np.mean(data.PP), np.std(data.PP)\n",
    "    df['PHYC'][i], df['PHYC_std'][i] = np.mean(data.PHYC), np.std(data.PHYC)\n",
    "    df['CHL'][i], df['CHL_std'][i] = np.mean(data.CHL), np.std(data.CHL)\n",
    "df.to_csv('Tara_CPlyase_loaded.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class='anchor' id='doc'></a>\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "<center>\n",
    "<h1> CMAP Online Documentation </h1>\n",
    "</center>\n",
    "\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "IPython.display.IFrame('https://simons-ocean-atlas-documentation.readthedocs.io/en/latest/index.html', width=1200, height=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class='anchor' id='discussion'></a>\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "<center>\n",
    "<h1> Open Discussion </h1>\n",
    "</center>\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "<ul style=\"list-style-type:circle\">\n",
    "  <li>What other public datasets should be ingested in the database?</li>\n",
    "  <li>Any sane/practical solution/suggestion to deal with variable naming convention (Ontology)!</li>\n",
    "  <li>Do you have any comment/concern about the suggested data structure?</li> \n",
    "  <li>How about the suggested metadata?</li> \n",
    "  <li>General: any questions about how to contribue in the project?</li>\n",
    "  <li>...</li> \n",
    "</ul>\n",
    "\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class='anchor' id='contact'></a>\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "<center>\n",
    "<h1> Contacts </h1>\n",
    "\n",
    "<ul style=\"list-style-type:none\">\n",
    "  <li>Norland Raphael Hagen (nrhagen@uw.edu)</li>  \n",
    "  <li>Mohammad Ashkezari(mdehghan@uw.edu)</li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "</center>\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
